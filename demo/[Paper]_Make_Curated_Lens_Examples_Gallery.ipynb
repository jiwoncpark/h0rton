{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corner\n",
    "import pandas as pd\n",
    "import torch\n",
    "from baobab.sim_utils import add_g1g2_columns\n",
    "from baobab.data_augmentation.noise_lenstronomy import NoiseModelNumpy\n",
    "import lenstronomy\n",
    "print(lenstronomy.__path__)\n",
    "import os\n",
    "from baobab.data_augmentation.noise_lenstronomy import get_noise_sigma2_lenstronomy\n",
    "import h0rton.tdlmc_utils as tdlmc_utils\n",
    "from h0rton.configs import TrainValConfig, TestConfig\n",
    "from h0rton.h0_inference import H0Posterior, plot_h0_histogram, h0_utils, plotting_utils\n",
    "from h0rton.trainval_data import XYData\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from lenstronomy.LensModel.lens_model import LensModel\n",
    "from lenstronomy.LensModel.lens_model_extensions import LensModelExtensions\n",
    "from lenstronomy.LensModel.Solver.lens_equation_solver import LensEquationSolver\n",
    "from lenstronomy.Cosmo.lens_cosmo import LensCosmo\n",
    "from lenstronomy.Util import constants\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "from  lenstronomy.Plots import lens_plot\n",
    "import lenstronomy.Util.util as util\n",
    "import lenstronomy.Util.simulation_util as sim_util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from lenstronomy.LensModel.lens_model_extensions import LensModelExtensions\n",
    "from lenstronomy.Data.imaging_data import ImageData\n",
    "from lenstronomy.Plots import plot_util\n",
    "import scipy.ndimage as ndimage\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from baobab.configs import BaobabConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rc('font', family='STIXGeneral', size=15)\n",
    "plt.rc('xtick', labelsize='medium')\n",
    "plt.rc('ytick', labelsize='medium')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('axes', linewidth=2, titlesize='large', labelsize='medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated lens examples gallery\n",
    "\n",
    "__Author:__ Ji Won Park (@jiwoncpark)\n",
    "\n",
    "__Created:__ 8/20/2020\n",
    "\n",
    "__Last run:__ 11/29/2020\n",
    "\n",
    "__Goals:__\n",
    "We compare the BNN-inferred, forward modeling, and precision ceiling H0 posteriors for four hand-picked lenses. The precision ceiling corresponds to the theoretical case of a perfectly known lens model. Any difference between the BNN-inferred posterior and the precision ceiling can be attributed to the lens model constraint.\n",
    "\n",
    "__Before_running:__\n",
    "1. Train the BNN, e.g.\n",
    "```bash\n",
    "python h0rton/train.py experiments/v2/train_val_cfg.json\n",
    "```\n",
    "\n",
    "2. Get inference results for the trained model and the precision ceiling, e.g.\n",
    "```bash\n",
    "python h0rton/infer_h0_mcmc_default.py experiments/v2/mcmc_default.json\n",
    "python h0rton/infer_h0_simple_mc_truth.py experiments/v0/simple_mc_default.json\n",
    "```\n",
    "\n",
    "3. Summarize the inference results, e.g.\n",
    "```bash\n",
    "python h0rton/summarize.py 2 mcmc_default\n",
    "python h0rton/summarize.py 0 mcmc_default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read in some inference configs and truth metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 200\n",
    "default_version_id = 3 # 1 HST orbit\n",
    "truth_version_id = 0 # precision ceiling\n",
    "\n",
    "default_version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(default_version_id)\n",
    "truth_version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(truth_version_id)\n",
    "default_summary = pd.read_csv(os.path.join(default_version_dir, 'summary.csv'), index_col=False).iloc[:n_test]\n",
    "truth_summary = pd.read_csv(os.path.join(truth_version_dir, 'summary.csv'), index_col=False).iloc[:n_test]\n",
    "true_H0 = 70.0\n",
    "true_Om0 = 0.3\n",
    "true_cosmo = FlatLambdaCDM(H0=true_H0, Om0=true_Om0)\n",
    "# Join with metadata to get n_img\n",
    "test_cfg_path = os.path.join(default_version_dir, 'mcmc_default.json')\n",
    "test_cfg = TestConfig.from_file(test_cfg_path)\n",
    "baobab_cfg = BaobabConfig.from_file(test_cfg.data.test_baobab_cfg_path)\n",
    "test_dir = baobab_cfg.out_dir\n",
    "metadata_path = os.path.join(test_dir, 'metadata.csv')\n",
    "meta = pd.read_csv(metadata_path, index_col=None)\n",
    "meta = add_g1g2_columns(meta)\n",
    "meta['id'] = meta.index\n",
    "default_summary = default_summary.merge(meta, on='id', how='inner', suffixes=['', '_y'])\n",
    "truth_summary = truth_summary.merge(meta, on='id', how='inner', suffixes=['', '_y'])\n",
    "# For getting noise kwargs\n",
    "train_val_cfg = TrainValConfig.from_file(test_cfg.train_val_config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to cover the whole range of H0 uncertainties, so select a lens from each quartile of H0 uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "std_quantiles = np.quantile(default_summary['H0_std'].values, [0.25, 0.5, 0.75, 1])\n",
    "#default_summary.sort_values('D_dt_sigma')\n",
    "#default_summary.iloc[43]\n",
    "#print(std_quantiles)\n",
    "#np.argmin(default_summary['H0_std'].values)\n",
    "example_lens_i = np.array([63, 37, 86, 43]) # IDs of four hand-picked lenses\n",
    "#np.digitize(default_summary.loc[example_lens_i, 'H0_std'].values, std_quantiles, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mean, standard_deviation, amplitude):\n",
    "    \"\"\"Gaussian PDF\"\"\"\n",
    "    return amplitude * np.exp( - ((x - mean) / standard_deviation) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell populates dictionaries associated with the BNN, forward modeling, and precision ceiling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in D_dt_samples from D_dt_dict files\n",
    "from scipy import stats\n",
    "\n",
    "n_test = 200 # number of lenses to visualize\n",
    "version_id = 3 # ID of the version folder in experiments\n",
    "prec_version_id = 0 # ID of the version folder corresponding to precision ceiling\n",
    "true_H0 = 70.0\n",
    "true_Om0 = 0.3\n",
    "\n",
    "version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(version_id)\n",
    "test_cfg_path = os.path.join(version_dir, 'mcmc_default.json')\n",
    "test_cfg = TestConfig.from_file(test_cfg_path)\n",
    "baobab_cfg = BaobabConfig.from_file(test_cfg.data.test_baobab_cfg_path)\n",
    "train_val_cfg = TrainValConfig.from_file(test_cfg.train_val_config_file_path)\n",
    "# Read in truth metadata\n",
    "metadata = pd.read_csv(os.path.join(baobab_cfg.out_dir, 'metadata.csv'), index_col=None, nrows=n_test)\n",
    "# Read in summary\n",
    "summary = pd.read_csv(os.path.join(version_dir, 'summary.csv'), index_col=None, nrows=n_test)\n",
    "\n",
    "ceiling_dir = os.path.join('/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(0), 'simple_mc_default')\n",
    "samples_dir = os.path.join(version_dir, 'mcmc_default')\n",
    "D_dt_dict_fnames = ['D_dt_dict_{0:04d}.npy'.format(lens_i) for lens_i in example_lens_i]\n",
    "oversampling = 20\n",
    "\n",
    "ceiling_samples_dict = {} # time delay precision ceiling\n",
    "ceiling_weights_dict = {} # time delay precision ceiling\n",
    "H0_samples_dict = {} # BNN-inferred H0 posterior\n",
    "fm_samples_dict = {} # forward modeling H0 posterior\n",
    "\n",
    "for i, lens_i in enumerate(example_lens_i):\n",
    "    truth_i = summary.iloc[lens_i]\n",
    "    # Populate ceiling dict\n",
    "    f_name_ceiling = 'h0_dict_{0:04d}.npy'.format(lens_i)\n",
    "    ceiling_dict = np.load(os.path.join(ceiling_dir, f_name_ceiling), allow_pickle=True).item()\n",
    "    ceiling_samples_dict[lens_i] = ceiling_dict['h0_samples']\n",
    "    ceiling_weights_dict[lens_i] = ceiling_dict['h0_weights']\n",
    "    # Populate BNN dict\n",
    "    f_name = 'D_dt_dict_{0:04d}.npy'.format(lens_i)\n",
    "    uncorrected_D_dt_samples = np.load(os.path.join(samples_dir, f_name), allow_pickle=True).item()['D_dt_samples'] # [old_n_samples,]\n",
    "    # Correct D_dt samples using k_ext\n",
    "    uncorrected_D_dt_samples = h0_utils.remove_outliers_from_lognormal(uncorrected_D_dt_samples, 3).reshape(-1, 1) # [n_samples, 1] \n",
    "    k_ext_rv = getattr(stats, test_cfg.kappa_ext_prior.dist)(**test_cfg.kappa_ext_prior.kwargs)\n",
    "    k_ext = k_ext_rv.rvs(size=[len(uncorrected_D_dt_samples), oversampling]) # [n_samples, oversampling]\n",
    "    if test_cfg.kappa_ext_prior.transformed:\n",
    "        D_dt_samples = (uncorrected_D_dt_samples*k_ext).flatten()\n",
    "    else:\n",
    "        D_dt_samples = (uncorrected_D_dt_samples/(1.0 - k_ext)).flatten() # [n_samples,]\n",
    "    # Convert D_dt into H0\n",
    "    cosmo_converter = h0_utils.CosmoConverter(truth_i['z_lens'], truth_i['z_src'], H0=true_H0, Om0=true_Om0)\n",
    "    H0_samples = cosmo_converter.get_H0(D_dt_samples)\n",
    "    H0_samples_dict[lens_i] = H0_samples\n",
    "    # Populate forward modeling dict\n",
    "    version_id = 11\n",
    "    fm_version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(version_id)\n",
    "    fm_samples_path = os.path.join(fm_version_dir, 'forward_modeling_{:d}'.format(lens_i), 'D_dt_dict_{0:04d}.npy'.format(lens_i))\n",
    "    fm_D_dt_samples = np.load(fm_samples_path, allow_pickle=True).item()['D_dt_samples']\n",
    "    fm_D_dt_samples = fm_D_dt_samples[int(fm_D_dt_samples.shape[0]*0.95):]\n",
    "    fm_D_dt_samples *= np.random.normal(1, 0.025, fm_D_dt_samples.shape)\n",
    "    fm_H0_samples = cosmo_converter.get_H0(fm_D_dt_samples)\n",
    "    fm_samples_dict[lens_i] = fm_H0_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the below cells take a while (~10 min each) to run because of the caustics computation. We enable two plots: one that includes the precision ceiling (the paper version) and the other that doesn't (talk version, to minimize confusion). The paper version first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "n_rows = 2\n",
    "n_cols = 4\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 8))\n",
    "\n",
    "\n",
    "# H0 histograms\n",
    "for col_i, lens_i in enumerate(example_lens_i):\n",
    "    axes[0, col_i].axvline(x=true_H0, linestyle='--', color='k', label='Truth = 70 km Mpc$^{-1}$ s$^{-1}$')\n",
    "    # Plot precision floor\n",
    "    truth_lens_info = truth_summary[truth_summary['id'] == lens_i].squeeze()\n",
    "    truth_H0_mean = truth_lens_info['H0_mean']\n",
    "    truth_H0_std = truth_lens_info['H0_std']\n",
    "    amp = 1.0/truth_H0_std/np.sqrt(2*np.pi)\n",
    "    popt = [truth_H0_mean, truth_H0_std, amp]\n",
    "    #truth_samples = np.random.normal(truth_H0_mean, truth_H0_std, 10000)\n",
    "    x_interval_for_fit = np.linspace(40, 100, 1000) \n",
    "    default_lens_info = default_summary[default_summary['id'] == lens_i].squeeze()\n",
    "    default_H0_mean = default_lens_info['H0_mean']\n",
    "    default_H0_std = default_lens_info['H0_std']\n",
    "    default_samples = np.random.normal(default_H0_mean, default_H0_std, 10000)\n",
    "    # Get max count\n",
    "    counts, bins = np.histogram(H0_samples_dict[lens_i], bins=40, range=[40, 100])\n",
    "    weight = np.ones_like(H0_samples_dict[lens_i])*amp/np.max(counts)\n",
    "    # Plot histogram of BNN samples\n",
    "    bins = np.linspace(40, 100, 30)\n",
    "    n, bins, _ = axes[0, col_i].hist(ceiling_samples_dict[lens_i], \n",
    "                            weights=ceiling_weights_dict[lens_i], \n",
    "                            bins=bins, alpha=1, range=[40, 100.0], edgecolor='tab:gray',\n",
    "                            histtype='step', density=True, linewidth=2,\n",
    "                            label='Time delay precision ceiling') \n",
    "    \n",
    "    fm_counts, fm_bins = np.histogram(fm_samples_dict[lens_i], bins=bins, range=[40, 100])\n",
    "    fm_weight = np.max(n)/np.max(fm_counts)\n",
    "    _ = axes[0, col_i].hist(fm_samples_dict[lens_i], \n",
    "                            #weights=np.ones_like(fm_samples_dict[lens_i])*fm_weight,\n",
    "                            bins=bins, alpha=0.8, density=True, color='#8ca252', range=[40, 100.0], \n",
    "                            edgecolor='#637939', histtype='stepfilled', linewidth=1.0, \n",
    "                            label='Forward modeling posterior') \n",
    "    bnn_counts, bnn_bins = np.histogram(H0_samples_dict[lens_i], bins=bins, range=[40, 100])\n",
    "    bnn_weight = np.max(n)/np.max(bnn_counts)\n",
    "    _ = axes[0, col_i].hist(H0_samples_dict[lens_i], \n",
    "                            #weights=np.ones_like(H0_samples_dict[lens_i])*bnn_weight,\n",
    "                            bins=bins, alpha=0.8, density=True, color='#d6616b', range=[40, 100.0], \n",
    "                            edgecolor='#843c39', histtype='stepfilled', linewidth=1.2, \n",
    "                            label='BNN-inferred posterior') \n",
    "    # Plot forward modeling\n",
    "    fm_stats = h0_utils.get_normal_stats(fm_samples_dict[lens_i])\n",
    "    # Plot histogram of time delay precision ceiling\n",
    "    #axes[0, col_i].plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), color='tab:gray', label='Time delay precision ceiling', lw=4)\n",
    "    \n",
    "    axes[0, col_i].set_xticks(np.arange(40, 100 + 5, 10))\n",
    "    axes[0, col_i].set_xticks(np.arange(40, 100 + 1, 1), minor=True)\n",
    "    axes[0, col_i].set_yticks([])\n",
    "    axes[0, col_i].set_xlabel('$H_0$ (km Mpc$^{-1}$ s$^{-1}$)', fontsize=20)\n",
    "    \n",
    "    subplot_legend_elements = [\n",
    "                                Patch(facecolor='#d6616b', edgecolor='#843c39', alpha=0.8, label='{:0.1f} $\\pm$ {:0.1f}'.format(default_H0_mean, default_H0_std)),\n",
    "                              Patch(facecolor='#8ca252', edgecolor='#637939', alpha=0.8, label='{:0.1f} $\\pm$ {:0.1f}'.format(fm_stats['mean'], fm_stats['std'])),\n",
    "    Patch(facecolor='white', edgecolor='tab:gray', linewidth=2, alpha=0.75, label='{:0.1f} $\\pm$ {:0.1f}'.format(truth_H0_mean, truth_H0_std)),]\n",
    "    subplot_legend = axes[0, col_i].legend(handles=subplot_legend_elements, loc=[0.68, 0.68], framealpha=1.0, fontsize=20)\n",
    "    axes[0, col_i].add_artist(subplot_legend)\n",
    "    \n",
    "global_legend = axes[0, 0].legend(bbox_to_anchor=(0.03, 1.23, n_cols + 1.15, 0.102), loc='upper center', ncol=4, mode=\"expand\", borderaxespad=-0.5, fontsize=20, frameon=False)\n",
    "\n",
    "axes[0, 0].add_artist(global_legend)\n",
    "axes[0, 0].set_ylabel('Density', fontsize=25)\n",
    "\n",
    "bp = baobab_cfg.survey_info.bandpass_list[0]\n",
    "exposure_time_factor = np.ones([1, 1, 1]) \n",
    "survey_object = baobab_cfg.survey_object_dict[bp]\n",
    "# Dictionary of SingleBand kwargs\n",
    "noise_kwargs = survey_object.kwargs_single_band()\n",
    "# Factor of effective exptime relative to exptime of the noiseless images\n",
    "exposure_time_factor[0, :, :] = train_val_cfg.data.eff_exposure_time[bp]/noise_kwargs['exposure_time']\n",
    "\n",
    "noise_kwargs.update(exposure_time=train_val_cfg.data.eff_exposure_time[bp])\n",
    "# Dictionary of noise models\n",
    "noise_model = NoiseModelNumpy(**noise_kwargs)\n",
    "\n",
    "# Noised images\n",
    "for col_i, lens_i in enumerate(example_lens_i):\n",
    "    lens_info = default_summary[default_summary['id'] == lens_i].squeeze()\n",
    "    img = np.load(os.path.join(test_dir, 'X_{0:07d}.npy'.format(int(lens_i))))\n",
    "    # Add noise\n",
    "    img *= exposure_time_factor\n",
    "    #noise_map = noise_model.get_noise_map(img)\n",
    "    #img += noise_map\n",
    "    img = np.squeeze(img)\n",
    "    # Transform\n",
    "    img = np.log1p(img)\n",
    "    # Overlay caustic, critical curves\n",
    "    lens_model = LensModel(lens_model_list=['PEMD', 'SHEAR'], cosmo=true_cosmo, z_lens=lens_info['z_lens'], z_source=lens_info['z_src'])\n",
    "    kwargs_lens = [{'theta_E': lens_info['lens_mass_theta_E'], 'gamma': lens_info['lens_mass_gamma'], 'center_x': lens_info['lens_mass_center_x'], 'center_y': lens_info['lens_mass_center_y'], 'e1': lens_info['lens_mass_e1'], 'e2': lens_info['lens_mass_e2']}, {'gamma1': lens_info['external_shear_gamma1'], 'gamma2': lens_info['external_shear_gamma2']}]\n",
    "    x_source = lens_info['src_light_center_x']\n",
    "    y_source = lens_info['src_light_center_y']\n",
    "    plotting_utils.lens_model_plot_custom(img, axes[1, col_i], lensModel=lens_model, kwargs_lens=kwargs_lens, sourcePos_x=x_source, sourcePos_y=y_source, point_source=True, with_caustics=True, deltaPix=0.08, numPix=64)\n",
    "    axes[1, col_i].axis('off')\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.3)\n",
    "\n",
    "#fig.savefig('../curated_gallery.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the talk version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "n_rows = 2\n",
    "n_cols = 4\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 8))\n",
    "\n",
    "\n",
    "# H0 histograms\n",
    "for col_i, lens_i in enumerate(example_lens_i):\n",
    "    axes[0, col_i].axvline(x=true_H0, linestyle='--', color='k', label='Truth = 70 km Mpc$^{-1}$ s$^{-1}$')\n",
    "    # Plot precision floor\n",
    "    truth_lens_info = truth_summary[truth_summary['id'] == lens_i].squeeze()\n",
    "    truth_H0_mean = truth_lens_info['H0_mean']\n",
    "    truth_H0_std = truth_lens_info['H0_std']\n",
    "    amp = 1.0/truth_H0_std/np.sqrt(2*np.pi)\n",
    "    popt = [truth_H0_mean, truth_H0_std, amp]\n",
    "    #truth_samples = np.random.normal(truth_H0_mean, truth_H0_std, 10000)\n",
    "    x_interval_for_fit = np.linspace(40, 100, 1000) \n",
    "    default_lens_info = default_summary[default_summary['id'] == lens_i].squeeze()\n",
    "    default_H0_mean = default_lens_info['H0_mean']\n",
    "    default_H0_std = default_lens_info['H0_std']\n",
    "    default_samples = np.random.normal(default_H0_mean, default_H0_std, 10000)\n",
    "    # Get max count\n",
    "    counts, bins = np.histogram(H0_samples_dict[lens_i], bins=40, range=[40, 100])\n",
    "    weight = np.ones_like(H0_samples_dict[lens_i])*amp/np.max(counts)\n",
    "    # Plot histogram of BNN samples\n",
    "    bins = np.linspace(40, 100, 30)\n",
    "    #n, bins, _ = axes[0, col_i].hist(ceiling_samples_dict[lens_i], \n",
    "    #                        weights=ceiling_weights_dict[lens_i], \n",
    "    #                        bins=bins, alpha=1, range=[40, 100.0], edgecolor='tab:gray',\n",
    "    #                        histtype='step', density=True, linewidth=2,\n",
    "    #                        label='Time delay precision ceiling') \n",
    "    \n",
    "    #fm_counts, fm_bins = np.histogram(fm_samples_dict[lens_i], bins=bins, range=[40, 100])\n",
    "    #fm_weight = np.max(n)/np.max(fm_counts)\n",
    "    _ = axes[0, col_i].hist(fm_samples_dict[lens_i], \n",
    "                            #weights=np.ones_like(fm_samples_dict[lens_i])*fm_weight,\n",
    "                            bins=bins, alpha=0.8, density=True, color='#8ca252', range=[40, 100.0], \n",
    "                            edgecolor='#637939', histtype='stepfilled', linewidth=1.0, \n",
    "                            label='Forward modeling posterior') \n",
    "    #bnn_counts, bnn_bins = np.histogram(H0_samples_dict[lens_i], bins=bins, range=[40, 100])\n",
    "    #bnn_weight = np.max(n)/np.max(bnn_counts)\n",
    "    _ = axes[0, col_i].hist(H0_samples_dict[lens_i], \n",
    "                            #weights=np.ones_like(H0_samples_dict[lens_i])*bnn_weight,\n",
    "                            bins=bins, alpha=0.8, density=True, color='#d6616b', range=[40, 100.0], \n",
    "                            edgecolor='#843c39', histtype='stepfilled', linewidth=1.2, \n",
    "                            label='BNN-inferred posterior') \n",
    "    # Plot forward modeling\n",
    "    fm_stats = h0_utils.get_normal_stats(fm_samples_dict[lens_i])\n",
    "    # Plot histogram of time delay precision ceiling\n",
    "    #axes[0, col_i].plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), color='tab:gray', label='Time delay precision ceiling', lw=4)\n",
    "    \n",
    "    axes[0, col_i].set_xticks(np.arange(40, 100 + 5, 10))\n",
    "    axes[0, col_i].set_xticks(np.arange(40, 100 + 1, 1), minor=True)\n",
    "    axes[0, col_i].set_yticks([])\n",
    "    axes[0, col_i].set_xlabel('$H_0$ (km Mpc$^{-1}$ s$^{-1}$)', fontsize=20)\n",
    "    \n",
    "    subplot_legend_elements = [\n",
    "                                Patch(facecolor='#d6616b', edgecolor='#843c39', alpha=0.8, label='{:0.1f} $\\pm$ {:0.1f}'.format(default_H0_mean, default_H0_std)),\n",
    "                              Patch(facecolor='#8ca252', edgecolor='#637939', alpha=0.8, label='{:0.1f} $\\pm$ {:0.1f}'.format(fm_stats['mean'], fm_stats['std'])),]\n",
    "    #Patch(facecolor='white', edgecolor='tab:gray', linewidth=2, alpha=0.75, label='{:0.1f} $\\pm$ {:0.1f}'.format(truth_H0_mean, truth_H0_std)),]\n",
    "    subplot_legend = axes[0, col_i].legend(handles=subplot_legend_elements, loc=[0.68, 0.68], framealpha=1.0, fontsize=20)\n",
    "    axes[0, col_i].add_artist(subplot_legend)\n",
    "    \n",
    "global_legend = axes[0, 0].legend(bbox_to_anchor=(0.03, 1.23, n_cols + 1.15, 0.102), loc='upper center', ncol=4, mode=\"expand\", borderaxespad=-0.5, fontsize=20, frameon=False)\n",
    "\n",
    "axes[0, 0].add_artist(global_legend)\n",
    "axes[0, 0].set_ylabel('Density', fontsize=25)\n",
    "\n",
    "bp = baobab_cfg.survey_info.bandpass_list[0]\n",
    "exposure_time_factor = np.ones([1, 1, 1]) \n",
    "survey_object = baobab_cfg.survey_object_dict[bp]\n",
    "# Dictionary of SingleBand kwargs\n",
    "noise_kwargs = survey_object.kwargs_single_band()\n",
    "# Factor of effective exptime relative to exptime of the noiseless images\n",
    "exposure_time_factor[0, :, :] = train_val_cfg.data.eff_exposure_time[bp]/noise_kwargs['exposure_time']\n",
    "\n",
    "noise_kwargs.update(exposure_time=train_val_cfg.data.eff_exposure_time[bp])\n",
    "# Dictionary of noise models\n",
    "noise_model = NoiseModelNumpy(**noise_kwargs)\n",
    "\n",
    "# Noised images\n",
    "for col_i, lens_i in enumerate(example_lens_i):\n",
    "    lens_info = default_summary[default_summary['id'] == lens_i].squeeze()\n",
    "    img = np.load(os.path.join(test_dir, 'X_{0:07d}.npy'.format(int(lens_i))))\n",
    "    # Add noise\n",
    "    img *= exposure_time_factor\n",
    "    #noise_map = noise_model.get_noise_map(img)\n",
    "    #img += noise_map\n",
    "    img = np.squeeze(img)\n",
    "    # Transform\n",
    "    img = np.log1p(img)\n",
    "    # Overlay caustic, critical curves\n",
    "    lens_model = LensModel(lens_model_list=['PEMD', 'SHEAR'], cosmo=true_cosmo, z_lens=lens_info['z_lens'], z_source=lens_info['z_src'])\n",
    "    kwargs_lens = [{'theta_E': lens_info['lens_mass_theta_E'], 'gamma': lens_info['lens_mass_gamma'], 'center_x': lens_info['lens_mass_center_x'], 'center_y': lens_info['lens_mass_center_y'], 'e1': lens_info['lens_mass_e1'], 'e2': lens_info['lens_mass_e2']}, {'gamma1': lens_info['external_shear_gamma1'], 'gamma2': lens_info['external_shear_gamma2']}]\n",
    "    x_source = lens_info['src_light_center_x']\n",
    "    y_source = lens_info['src_light_center_y']\n",
    "    #plotting_utils.lens_model_plot_custom(img, axes[1, col_i], lensModel=lens_model, kwargs_lens=kwargs_lens, sourcePos_x=x_source, sourcePos_y=y_source, point_source=True, with_caustics=True, deltaPix=0.08, numPix=64)\n",
    "    axes[1, col_i].axis('off')\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.3)\n",
    "\n",
    "#fig.savefig('../curated_cwp.png', dpi=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (baobab)",
   "language": "python",
   "name": "baobab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
