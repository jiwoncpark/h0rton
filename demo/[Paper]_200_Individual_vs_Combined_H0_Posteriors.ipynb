{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import scipy.stats as stats\n",
    "import lenstronomy\n",
    "print(lenstronomy.__path__)\n",
    "from h0rton.configs import TrainValConfig, TestConfig\n",
    "from baobab.configs import BaobabConfig\n",
    "import h0rton.tdlmc_utils as tdlmc_utils\n",
    "import baobab.sim_utils as sim_utils\n",
    "from h0rton.h0_inference import h0_utils, plotting_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Plotting params\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rc('font', family='STIXGeneral', size=20)\n",
    "plt.rc('xtick', labelsize='medium')\n",
    "plt.rc('ytick', labelsize='medium')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('axes', linewidth=2, titlesize='large', labelsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual vs. combined H0 posteriors\n",
    "\n",
    "__Author:__ Ji Won Park (@jiwoncpark)\n",
    "\n",
    "__Created:__ 8/20/2020\n",
    "\n",
    "__Last run:__ 11/29/2020\n",
    "\n",
    "__Goals:__\n",
    "The true power of BNN lens modeling lies in many-lens joint inference. It compensates for lower precision on individual lenses (when compared to traditional forward modeling) by enabling rapid modeling of 100s of lenses. In this notebook, we overlay the 200 individual H0 posteriors with the combined H0 posterior to illustrate this advantage.\n",
    "\n",
    "__Before_running:__\n",
    "1. Train the BNN, e.g.\n",
    "```bash\n",
    "python h0rton/train.py experiments/v2/train_val_cfg.json\n",
    "```\n",
    "\n",
    "2. Get inference results for the trained model and the precision ceiling, e.g.\n",
    "```bash\n",
    "python h0rton/infer_h0_mcmc_default.py experiments/v2/mcmc_default.json\n",
    "python h0rton/infer_h0_simple_mc_truth.py experiments/v0/simple_mc_default.json\n",
    "```\n",
    "\n",
    "3. Summarize the inference results, e.g.\n",
    "```bash\n",
    "python h0rton/summarize.py 2 mcmc_default\n",
    "python h0rton/summarize.py 0 mcmc_default\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 200 # number of lenses to visualize\n",
    "version_id = 3 # ID of the version folder in experiments\n",
    "prec_version_id = 0 # ID of the version folder corresponding to precision ceiling\n",
    "true_H0 = 70.0\n",
    "true_Om0 = 0.3\n",
    "# Read in inference config\n",
    "version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(version_id)\n",
    "test_cfg_path = os.path.join(version_dir, 'mcmc_default.json')\n",
    "test_cfg = TestConfig.from_file(test_cfg_path)\n",
    "baobab_cfg = BaobabConfig.from_file(test_cfg.data.test_baobab_cfg_path)\n",
    "train_val_cfg = TrainValConfig.from_file(test_cfg.train_val_config_file_path)\n",
    "# Read in truth metadata\n",
    "metadata = pd.read_csv(os.path.join(baobab_cfg.out_dir, 'metadata.csv'), index_col=None, nrows=n_test)\n",
    "# Read in summary\n",
    "summary = pd.read_csv(os.path.join(version_dir, 'summary.csv'), index_col=None, nrows=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify 1-sigma, 2-sigma, and beyond\n",
    "summary['standard_z'] = (70 - summary['H0_mean'])/summary['H0_std']\n",
    "summary['class_z'] = np.digitize(np.abs(summary['standard_z'].values), [1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dir = os.path.join(version_dir, 'mcmc_default')\n",
    "D_dt_dict_fnames = [f for f in os.listdir(samples_dir) if f.startswith('D_dt_dict')]\n",
    "D_dt_dict_fnames.sort()\n",
    "oversampling = 20 # oversampling factor for convolving D_dt with kappa_ext prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in D_dt_samples from D_dt_dict files\n",
    "H0_samples_dict = {}\n",
    "kde_dict = {}\n",
    "coverage_probs = {}\n",
    "for i, f_name in enumerate(D_dt_dict_fnames):\n",
    "    # Read in D_dt samples using lens identifier\n",
    "    lens_i = int(os.path.splitext(f_name)[0].split('D_dt_dict_')[1])\n",
    "    uncorrected_D_dt_samples = np.load(os.path.join(samples_dir, f_name), allow_pickle=True).item()['D_dt_samples'] # [old_n_samples,]\n",
    "    # Read in redshifts and coverage sigma\n",
    "    class_z, z, z_lens, z_src = summary.loc[lens_i, ['class_z', 'z', 'z_lens', 'z_src']]\n",
    "    \n",
    "    # Correct D_dt samples using k_ext\n",
    "    uncorrected_D_dt_samples = h0_utils.remove_outliers_from_lognormal(uncorrected_D_dt_samples, 3).reshape(-1, 1) # [n_samples, 1] \n",
    "    k_ext_rv = getattr(stats, test_cfg.kappa_ext_prior.dist)(**test_cfg.kappa_ext_prior.kwargs)\n",
    "    k_ext = k_ext_rv.rvs(size=[len(uncorrected_D_dt_samples), oversampling]) # [n_samples, oversampling]\n",
    "    if test_cfg.kappa_ext_prior.transformed:\n",
    "        D_dt_samples = (uncorrected_D_dt_samples*k_ext).flatten()\n",
    "    else:\n",
    "        D_dt_samples = (uncorrected_D_dt_samples/(1.0 - k_ext)).flatten() # [n_samples,]\n",
    "    \n",
    "    # Convert D_dt into H0\n",
    "    cosmo_converter = h0_utils.CosmoConverter(z_lens, z_src, H0=true_H0, Om0=true_Om0)\n",
    "    H0_samples = cosmo_converter.get_H0(D_dt_samples)\n",
    "    H0_samples_dict[lens_i] = H0_samples\n",
    "    \n",
    "    # Get KDE approximation\n",
    "    kde = stats.gaussian_kde(H0_samples)\n",
    "    kde_dict[lens_i] = kde\n",
    "    coverage_probs[lens_i] = kde.integrate_box_1d(low=-np.inf, high=70.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design a custom colormap because we don't want the colors for the ~99\\% percentile lenses to be too light as to be indistinguishable from the white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmapIn='jet', minval=0.0, maxval=1.0, n=100):\n",
    "    '''truncate_colormap(cmapIn='jet', minval=0.0, maxval=1.0, n=100)'''    \n",
    "    cmapIn = plt.get_cmap(cmapIn)\n",
    "\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmapIn.name, a=minval, b=maxval),\n",
    "        cmapIn(np.geomspace(minval, maxval, n)))\n",
    "\n",
    "    arr = np.linspace(0, 50, 100).reshape((10, 10))\n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "    ax[0].imshow(arr, interpolation='nearest', cmap=cmapIn)\n",
    "    ax[1].imshow(arr, interpolation='nearest', cmap=new_cmap)\n",
    "    plt.show()\n",
    "    return new_cmap\n",
    "\n",
    "viridis_mod = truncate_colormap(cmapIn='viridis', minval=.01, maxval=.925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_to_scalar(cdf):\n",
    "    \"\"\"Convert a CDF value to a matplotlib.cm scalar\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cdf : float\n",
    "        the CDF of the individual H0 posterior evaluated at the truth,\n",
    "        e.g. for 0.5 + 0.3415 if truth is exactly 1 sigma away from mean\n",
    "        \n",
    "    \"\"\"\n",
    "    return np.abs(cdf - 0.5)/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# CDF change from 0.5 at 1, 2, 3 sigmas away from mean,\n",
    "# for a standard normal dist\n",
    "standard_normal_p = np.array([0.3413, 0.4773, 0.4986])\n",
    "cmap = viridis_mod#cm.viridis\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "# Tune alpha based on standard z score\n",
    "class_z_to_alpha = {0: 0.4, 1: 0.4, 2: 1.0, 3: 1.0}\n",
    "#H0_grid = np.linspace(10, 130, 400)\n",
    "for i, lens_i in enumerate(H0_samples_dict):\n",
    "    class_z, z_lens, z_src = summary.loc[lens_i, ['class_z', 'z_lens', 'z_src']]\n",
    "    cbar_scalar = cdf_to_scalar(coverage_probs[lens_i])\n",
    "    # Plot individual H0 posterior\n",
    "    H0_samples = H0_samples_dict[lens_i]\n",
    "    H0_grid = np.histogram_bin_edges(H0_samples, bins='scott')\n",
    "    #if np.abs(H0_grid[0] - 20.0) > 5:\n",
    "    #    H0_grid = 50\n",
    "    ax.hist(H0_samples, bins=H0_grid, histtype='step', alpha=class_z_to_alpha[class_z], density=True, color=cmap(cbar_scalar))#color_map_color(cbar_scalar))\n",
    "# Plot combined H0 posterior\n",
    "h0_samples = np.load(os.path.join('/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(version_id), 'combined_H0_summary_kde.npy'))\n",
    "combined_bin = np.histogram_bin_edges(h0_samples, bins='scott')\n",
    "combined_center, combined_lower_sig, combined_upper_sig = tuple(np.quantile(h0_samples, [0.5, 0.16, 0.84]))\n",
    "print(combined_center, combined_lower_sig, combined_upper_sig)\n",
    "\n",
    "ax.hist(h0_samples, bins=combined_bin, color='#d6616b', edgecolor='#843c39', histtype='stepfilled', density=False, weights=np.ones_like(h0_samples)/3100.0, linewidth=1)\n",
    "# Plot truth\n",
    "plt.axvline(70, color='k', lw=2, linestyle='--')\n",
    "\n",
    "# Plot colorbar for individual H0 posterior\n",
    "norm = matplotlib.colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "tick_cdf = standard_normal_p + 0.5 # CDF at mean + 1, 2, 3 std\n",
    "cbar_ticks = cdf_to_scalar(tick_cdf)\n",
    "print(tick_cdf)\n",
    "print(cbar_ticks)\n",
    "cbar_ticklabels = [r'{:0.1f}\\%'.format((c - 0.5)*2.0*100) for c in tick_cdf]\n",
    "cbar = fig.colorbar(cm.ScalarMappable(cmap=cmap, norm=norm), orientation='vertical', ticks=cbar_ticks)\n",
    "#cbar.set_clim([0.0, 0.7])\n",
    "#cb.ax.plot([0, 1], [rms]*2, 'w') \n",
    "plt.xlim([10, 130])\n",
    "plt.ylim([0.0, 0.12])\n",
    "cbar.set_label(\"Individual\", rotation=270, fontsize=20)\n",
    "cbar.ax.set_yticklabels(cbar_ticklabels, fontsize=15)\n",
    "\n",
    "legend_elements = [\n",
    "                   Line2D([0], [0], color='k', lw=2, alpha=1, linestyle='--', label='Truth = 70'),\n",
    "                Patch(facecolor='#d6616b', edgecolor='#843c39', lw=1, label='Combined (rescaled) = $69.7^{+0.5}_{-0.4}$ (0.7\\% precision)'),\n",
    "    Patch(facecolor='white', edgecolor=cmap(standard_normal_p[2] + 0.5), label=r'100.0\\% of lenses have truth within 99.7\\% C.I.'),\n",
    "    Patch(facecolor='white', edgecolor=cmap(standard_normal_p[1] + 0.5), label=r'94.5\\% of lenses have truth within 95.5\\% C.I.'),\n",
    "    Patch(facecolor='white', edgecolor=cmap(standard_normal_p[0] + 0.5), label=r'64.5\\% of lenses have truth within 68.3\\% C.I.'),\n",
    "               ]\n",
    "ax.legend(handles=legend_elements, fontsize=15)\n",
    "\n",
    "plt.xlabel(\"$H_0$ (km Mpc$^{-1}$ s$^{-1}$)\", fontsize=25)\n",
    "plt.ylabel(\"Density\", fontsize=25)\n",
    "plt.xticks(np.arange(10, 130 + 1, 10), fontsize=20)\n",
    "\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.grid(axis=\"x\", color=\"black\", alpha=.5,  linestyle='dotted')\n",
    "ax.grid(axis=\"x\", color=\"black\", which='minor', alpha=.2,  linestyle=(0, (1, 1)))\n",
    "#plt.tight_layout()\n",
    "#fig.savefig('../overlaid_200_posteriors.png', dpi=100, bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_values = np.array(list(coverage_probs.values()))\n",
    "cdf_values_binned = np.digitize(np.abs(cdf_values - 0.5), list(standard_normal_p) + [0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, _, _ = plt.hist(cdf_values_binned)\n",
    "print(counts/200)\n",
    "print(counts)\n",
    "print((counts[0] + counts[-1] + 60)/200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (baobab)",
   "language": "python",
   "name": "baobab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
