{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lenstronomy\n",
    "print(lenstronomy.__path__)\n",
    "from h0rton.configs import TrainValConfig, TestConfig\n",
    "from baobab.configs import BaobabConfig\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import h0rton.tdlmc_utils as tdlmc_utils\n",
    "import baobab.sim_utils as sim_utils\n",
    "\n",
    "import numba\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Plotting params\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rc('font', family='STIXGeneral', size=20)\n",
    "plt.rc('xtick', labelsize='medium')\n",
    "plt.rc('ytick', labelsize='medium')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('axes', linewidth=2, titlesize='large', labelsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and evaluation\n",
    "\n",
    "__Author:__ Ji Won Park (@jiwoncpark)\n",
    "\n",
    "__Created:__ 8/20/2020\n",
    "\n",
    "__Last run:__ 11/29/2020\n",
    "\n",
    "__Goals:__\n",
    "We perform model selection and evaluation based on the validation set.\n",
    "\n",
    "__Before_running:__\n",
    "1. Train the BNN for all exposure times and dropout rates, e.g. for an exposure time of 2 HST orbits (v2) and dropout rate of 0.001,\n",
    "```bash\n",
    "python h0rton/train.py experiments/v2/train_val_cfg.json\n",
    "```\n",
    "\n",
    "2. Get BNN predictions on the validation set for various exposure times and dropout rates, e.g.\n",
    "```bash\n",
    "python h0rton/infer_mcmc_default.py experiments/v2/mcmc_default_samples_drop=0.001.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Model selection (choosing the dropout rate)](#dropout)\n",
    "2. [Model evaluation (accuracy and precision)](#model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model selection (choosing the dropout rate) <a name=\"dropout\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bnn_predictions(version_id, dropout='drop=0.001'):\n",
    "    \"\"\"Stores the BNN predictions for each exposure time and dropout rate\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    version_id : int\n",
    "        version ID corresponding to the orbit\n",
    "    dropout : str\n",
    "        identifying string for dropout. Default: 'drop=0.001'\n",
    "    \n",
    "    \"\"\"\n",
    "    n_val = 512 # number of validation lenses\n",
    "    version_dir = '/home/jwp/stage/sl/h0rton/experiments/v{:d}'.format(version_id)\n",
    "    test_cfg_path = os.path.join(version_dir, 'mcmc_default_samples_{:s}.json'.format(dropout))\n",
    "    test_cfg = TestConfig.from_file(test_cfg_path)\n",
    "    baobab_cfg = BaobabConfig.from_file(test_cfg.data.test_baobab_cfg_path)\n",
    "    train_val_cfg = TrainValConfig.from_file(test_cfg.train_val_config_file_path)\n",
    "    # Read in truth metadata of the validation set\n",
    "    truth_info = pd.read_csv(os.path.join(baobab_cfg.out_dir, 'metadata.csv'), index_col=None, nrows=n_val)\n",
    "    # Assign lens ID based on the row index of metadata, for merging with summary\n",
    "    truth_info['id'] = truth_info.index\n",
    "    # Note that metadata stores the absolute source position, so get relative to lens center\n",
    "    truth_info['src_light_center_x'] -= truth_info['lens_mass_center_x']\n",
    "    truth_info['src_light_center_y'] -= truth_info['lens_mass_center_y']\n",
    "    # Read in the MC dropout BNN samples \n",
    "    samples = np.load('/home/jwp/stage/sl/h0rton/experiments/v{:d}/mcmc_default_samples_val_{:s}/samples.npy'.format(version_id, dropout))\n",
    "    # Merge MC dropout axis with samples per dropout\n",
    "    n_lenses = samples.shape[0]\n",
    "    mcmc_Y_dim = samples.shape[-1]\n",
    "    samples = samples.transpose(0, 3, 1, 2).reshape([n_lenses, mcmc_Y_dim, -1])[:n_val] \n",
    "    # Store BNN predictions (expected value and spread) as dataframes\n",
    "    pred_mean = np.mean(samples, axis=-1) # expected value\n",
    "    pred_std = np.std(samples, axis=-1)\n",
    "    params_to_remove = [] #'src_light_R_sersic'] \n",
    "    mcmc_Y_cols = [col for col in train_val_cfg.data.Y_cols if col not in params_to_remove]\n",
    "    pred_mean = pd.DataFrame(pred_mean, columns=mcmc_Y_cols)\n",
    "    pred_std = pd.DataFrame(pred_std, columns=mcmc_Y_cols)\n",
    "    pred_mean = sim_utils.add_qphi_columns(pred_mean)\n",
    "    pred_mean = sim_utils.add_gamma_psi_ext_columns(pred_mean)\n",
    "    truth_info = sim_utils.add_g1g2_columns(truth_info)\n",
    "    return pred_std, pred_mean, truth_info, samples, mcmc_Y_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = {} # BNN-predicted uncertainty\n",
    "mean = {} # BNN-predicted center\n",
    "truth = {} # True parameter value\n",
    "samples = {} # BNN posterior samples\n",
    "orbit_to_ver = dict(zip([0.5, 1, 2], [4, 3, 2])) # maps HST orbit to version folder\n",
    "orbits_available = [0.5, 1, 2] # HST orbits\n",
    "dropouts_available = ['no_dropout', 'drop=0.001', 'drop=0.005']\n",
    "dropout_label_to_dropout_float = dict(zip(dropouts_available, [0.0, 0.001, 0.005]))\n",
    "# Populate std, mean, truth, samples dicts with structure dict[orbit][dropout_rate]\n",
    "for o in orbits_available:\n",
    "    std[o] = {}\n",
    "    mean[o] = {}\n",
    "    truth[o] = {}\n",
    "    samples[o] = {}\n",
    "    for d in dropouts_available:\n",
    "        simple_std, simple_mean, simple_truth, simple_samples, mcmc_Y_cols = get_bnn_predictions(version_id=orbit_to_ver[o], dropout=d)\n",
    "        std[o][d] = simple_std\n",
    "        mean[o][d] = simple_mean\n",
    "        truth[o][d] = simple_truth # FIXME: truth doesn't vary with dropout or orbit\n",
    "        samples[o][d] = simple_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the 2D calibration plot we defined in Wagner-Carena et al 2020. The metric asks: for a given percentage of the BNN posterior probability volume $p_X$, what percentage of the samples, $p_Y$, contains the truth within this volume? If the posterior is perfectly calibrated, we would expect $p_X$ of the samples to encompass the truth $p_Y = p_X$ of the time, for every value of $p_X$. We can apply this metric on the validation set as a whole by averaging the $p_Y$ values evaluated on individual lenses:\n",
    "\n",
    "$$p_{ Y}^{\\rm val}(p_X) = \\frac{1}{N^{\\rm val}} \\sum_{k=1}^{N^{\\rm val}} {1}\\left\\{ \\frac{ \\sum_{n=1}^N  {1}\\left\\{d(\\xi_n^{(k)}) < d(\\xi_{\\rm true}^{(k)})\\right\\}}{N} < p_X \\right\\}$$\n",
    "\n",
    "where:\n",
    "- $k$ indexes the validation lenses;\n",
    "- $\\{\\xi_n^{(k)}\\}_{n=1}^N$ refers to the $N$ parameter samples drawn from the BNN posterior for some lens $k$;\n",
    "- $1\\{\\cdot\\}$ is an indicator function that evaluates to 1 when the argument is true and 0 otherwise; and\n",
    "- $d(\\xi)$ is a measure of distance of a particular point $\\xi$ from the posterior predictive mean given the posterior width.\n",
    "\n",
    "There are many choices for the distance measure $d$. Park et al 2020 defines $d$ as the distance of $\\xi$ from the posterior predictive mean given the posterior width *for that lens*. Instead, Wagner-Carena et al 2020 approximated $d(\\xi)$ as the position of $\\xi$ given the covariance matrix of the *training set* (the interim prior), out of speed considerations. The results aren't too different but we enable both options here, via `get_p_Y_val_mahalanobis` (Park et al 2020) and `get_p_Y_val_approx_mahalanobis` (Wagner-Carena et al 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(predicted, true):\n",
    "    \"\"\"Get the median absolute error, or median(|predicted - true|)\"\"\"\n",
    "    return np.median(np.abs(predicted - true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_bias(predicted_mean, predicted_std, true):\n",
    "    \"\"\"Get the error (bias) for a parameter inversely weighted by the BNN uncertainty\"\"\"\n",
    "    weighted_bias = np.average(predicted_mean - true, weights=1.0/predicted_std**2.0)\n",
    "    weighted_bias_spread = np.average((predicted_mean - true - weighted_bias)**2.0, weights=1.0/predicted_std**2.0)**0.5\n",
    "    return weighted_bias, weighted_bias_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_Y_val_approx_mahalanobis(post_samples, y_mean, y_truth, cov):\n",
    "    \"\"\"Calculate the percentage of draws from the predicted distribution that\n",
    "    encompasses the truth, for all of the examples in the validation set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    post_samples : np.array of shape [n_samples, n_lenses, Y_dim]\n",
    "        BNN posterior samples\n",
    "    y_mean : np.array of shape [n_lenses, Y_dim]\n",
    "        Central prediction to use in the distance calculation\n",
    "    y_truth: np.array of shape [n_lenses, Y_dim]\n",
    "        True parameter values\n",
    "    cov : np.array of shape []\n",
    "        Covariance matrix to use in the distance calculation\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from https://github.com/swagnercarena/ovejero\n",
    "    \n",
    "    \"\"\"\n",
    "    @numba.njit\n",
    "    def approx_mahalanobis(dif, cov):\n",
    "        \"\"\"Metric used in Wagner-Carena et al 2020\"\"\"\n",
    "        d_metric = np.zeros(dif.shape[0:2])\n",
    "        for i in range(d_metric.shape[0]):\n",
    "            for j in range(d_metric.shape[1]):\n",
    "                d_metric[i, j] = np.dot(dif[i, j], np.dot(cov, dif[i, j]))\n",
    "        return d_metric\n",
    "    p_Y_val = (approx_mahalanobis(post_samples-y_mean, cov) < approx_mahalanobis(np.expand_dims(y_truth-y_mean, axis=0), cov))\n",
    "    p_Y_val = np.mean(p_Y_val, axis=0)\n",
    "    return p_Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_Y_val_mahalanobis(post_samples, y_truth):\n",
    "    \"\"\"Calculate the percentage of draws from the predicted distribution that\n",
    "    encompasses the truth, for all of the examples in the validation set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    post_samples : np.array of shape [n_samples, n_lenses, Y_dim]\n",
    "        BNN posterior samples\n",
    "    y_mean : np.array of shape [n_lenses, Y_dim]\n",
    "        Central prediction to use in the distance calculation\n",
    "    y_truth: np.array of shape [n_lenses, Y_dim]\n",
    "        True parameter values\n",
    "        \n",
    "    \"\"\"\n",
    "    # The metric for the distance calculation. Using numba for speed.\n",
    "    @numba.njit\n",
    "    def mahalanobis(dif, prec):\n",
    "        \"\"\"Metric used in Park et al 2020\"\"\"\n",
    "        d_metric = np.zeros(dif.shape[0:2]) # [n_samples, n_lenses]\n",
    "        for j in range(d_metric.shape[1]):\n",
    "            for i in range(d_metric.shape[0]):\n",
    "                d_metric[i, j] = np.dot(dif[i, j], np.dot(prec[:, :, j], dif[i, j]))\n",
    "        return d_metric\n",
    "    \n",
    "    n_samples, n_lenses, Y_dim = post_samples.shape\n",
    "    p_Y_val = np.empty((n_samples, n_lenses))\n",
    "    y_mean = np.mean(post_samples, axis=0) # [n_lenses, Y_dim]\n",
    "    prec_val = np.empty((Y_dim, Y_dim, n_lenses)) # precision matrix of all lenses\n",
    "    for lens_i in range(y_truth.shape[0]): # loop over the lenses\n",
    "        prec_val[:, :, lens_i] = np.inv(np.cov(post_samples[:, lens_i, :])) # [Y_dim, Y_dim]\n",
    "    p_Y_val = (mahalanobis(post_samples-y_mean, prec_val) < mahalanobis(np.expand_dims(y_truth-y_mean, axis=0), prec_val)) # [n_samples, n_lenses]\n",
    "    p_Y_val = np.mean(p_Y_val, axis=0)\n",
    "    return p_Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the covariance matrix from the training set\n",
    "train_truth = pd.read_csv('/home/jwp/stage/sl/h0rton/v7_train_prior=DiagonalCosmoBNNPrior_seed=1113/metadata.csv', index_col=None)\n",
    "train_truth = sim_utils.add_g1g2_columns(train_truth) # convert gamma_ext, psi_ext to g1, g2\n",
    "train_cov = np.cov(train_truth[mcmc_Y_cols].values.T) # [Y_dim, Y_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration(post_samples, y_mean, y_truth, cov, color_map=[\"#377eb8\", \"#4daf4a\"], n_perc_points=20, figure=None, ls='--', legend=None, show_plot=True, block=True, title=None, dpi=200):\n",
    "    \"\"\"Plot the calibration metric for a grid of p_X percentages, with error bars\n",
    "    obtained through jackknife sampling\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    See the docstring for `get_p_Y_val_approx_mahalanobis`.\n",
    "    n_perc_points : int\n",
    "        Grid size of p_X (probability volume) to compare p_Y against\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Adapted from https://github.com/swagnercarena/ovejero\n",
    "    \n",
    "    \"\"\"\n",
    "    p_Y_val = get_p_Y_val_approx_mahalanobis(post_samples, y_mean, y_truth, cov=cov)\n",
    "\n",
    "    # Plot what percentage of images have at most x% of draws with\n",
    "    # p(draws)>p(true).\n",
    "    percentages = np.linspace(0.0, 1.0, n_perc_points)\n",
    "    p_images = np.zeros_like(percentages)\n",
    "    if figure is None:\n",
    "        fig = plt.figure(figsize=(8,8), dpi=dpi)\n",
    "        plt.plot(percentages, percentages, c=color_map[0], ls='--', label=legend[0])\n",
    "    else:\n",
    "        fig = figure\n",
    "\n",
    "    # We'll estimate the uncertainty in our plot using a jacknife method.\n",
    "    p_images_jn = np.zeros((len(p_Y_val), n_perc_points))\n",
    "    for pi in range(n_perc_points):\n",
    "        percent = percentages[pi]\n",
    "        p_images[pi] = np.mean(p_Y_val<=percent)\n",
    "        for ji in range(len(p_Y_val)):\n",
    "            samp_p_Y_val = np.delete(p_Y_val, ji)\n",
    "            p_images_jn[ji,pi] = np.mean(samp_p_Y_val<=percent)\n",
    "    # Estimate the standard deviation from the jacknife\n",
    "    p_Y_val_std = np.sqrt((len(p_Y_val)-1)*np.mean(np.square(p_images_jn - np.mean(p_images_jn,axis=0)), axis=0))\n",
    "    plt.plot(percentages, p_images, c=color_map[1], ls=ls, label=legend[1])\n",
    "    # Plot the 1 sigma contours from the jacknife estimate to get an idea of our sample variance.\n",
    "    plt.fill_between(percentages, p_images+p_Y_val_std, p_images-p_Y_val_std, color=color_map[1], alpha=0.2)\n",
    "    if figure is None:\n",
    "        plt.grid(True, ls='dotted', alpha=0.5)\n",
    "        plt.xlabel(r'Fraction of posterior volume = $p_X$', fontsize=15)\n",
    "        plt.ylabel(r'Fraction of validation lenses with truth in the volume = $p_Y^{\\mathrm{val}}$', fontsize=15)\n",
    "        plt.text(-0.03, 1,'Underconfident', fontsize=15)\n",
    "        plt.text(0.80, 0,'Overconfident', fontsize=15)\n",
    "    if legend is None:\n",
    "        plt.legend(['Perfect calibration','Network Calibration'], loc=loc)\n",
    "    else:\n",
    "        plt.legend(fontsize=15, loc=9)\n",
    "    if show_plot:\n",
    "        plt.show(block=block)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Colors for each exposure time (in HST orbits)\n",
    "colors_dict = dict(zip([2, 1, 0.5], ['#880519', '#c04546', '#f97978']))\n",
    "dropout_to_linestyle = dict(zip([0, 0.001, 0.005], ['dotted', 'solid', 'dashdot']))\n",
    "dropout_to_label = dict(zip([0, 0.001, 0.005], [r'$p_{\\rm drop} = 0\\%$', r'$p_{\\rm drop} = 0.1\\%$', r'$p_{\\rm drop} = 0.5\\%$']))\n",
    "\n",
    "show_orbit = 0.5 # orbit for which the dropouts are compared\n",
    " \n",
    "first_dropout = dropouts_available[0]\n",
    "fig = plot_calibration(post_samples=np.transpose(samples[show_orbit][first_dropout], [2, 0, 1]), \n",
    "                       y_mean=mean[show_orbit][first_dropout][mcmc_Y_cols].values,\n",
    "                       y_truth=truth[show_orbit][first_dropout][mcmc_Y_cols].values,\n",
    "                       cov=train_cov, \n",
    "                       show_plot=False,\n",
    "                       ls = dropout_to_linestyle[dropout_label_to_dropout_float[first_dropout]],\n",
    "                       color_map=['tab:gray'] + [colors_dict[show_orbit]],\n",
    "                       legend=['Perfect calibration', dropout_to_label[dropout_label_to_dropout_float[first_dropout]]])\n",
    "\n",
    "\n",
    "for d in dropouts_available[1:]:\n",
    "    fig = plot_calibration(post_samples=np.transpose(samples[show_orbit][d], [2, 0, 1]), \n",
    "                       y_mean=mean[show_orbit][d][mcmc_Y_cols].values,\n",
    "                       y_truth=truth[show_orbit][d][mcmc_Y_cols].values,\n",
    "                       cov=train_cov, \n",
    "                           figure=fig,\n",
    "                       show_plot=False,\n",
    "                       color_map=['tab:gray'] + [colors_dict[show_orbit]],\n",
    "                           ls = dropout_to_linestyle[dropout_label_to_dropout_float[d]],\n",
    "                       legend=['Perfect calibration', dropout_to_label[dropout_label_to_dropout_float[d]]])\n",
    "#fig.savefig('../plots/calibration.png', bbox_inches='tight', pad_inches=0, dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model evaluation  <a name=\"model_eval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dropout = 'drop=0.001'\n",
    "\n",
    "for param in ['lens_mass_center_x', 'src_light_center_x', 'lens_mass_center_y', 'src_light_center_y', 'lens_mass_gamma', 'lens_mass_theta_E', 'lens_mass_e1', 'lens_mass_e2', 'external_shear_gamma1', 'external_shear_gamma2', 'src_light_R_sersic']:\n",
    "    print(param)\n",
    "    for o in orbits_available:\n",
    "        print(\"HST orbit(s): \", o)\n",
    "        print(\"MAD: \", get_mad(mean[o][chosen_dropout][param].values, truth[o][chosen_dropout][param].values))\n",
    "        print(\"bias: \", get_weighted_bias(mean[o][chosen_dropout][param].values, std[o][chosen_dropout][param].values, truth[o][chosen_dropout][param].values))\n",
    "        print(\"precision: \", np.median(std[o][chosen_dropout][param].values))\n",
    "        #print(\"h0rton: \", get_mad(h0rton_mean[param].values, h0rton_truth[param].values))\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take min_img_conf number of doubles/quads from the 512 validation lenses\n",
    "\n",
    "chosen_orbit = 2\n",
    "min_img_conf = min(mean[chosen_orbit][chosen_dropout].loc[truth[chosen_orbit][chosen_dropout]['n_img'] == 2].shape[0], mean[chosen_orbit][chosen_dropout].loc[truth[chosen_orbit][chosen_dropout]['n_img'] == 4].shape[0])\n",
    "print(min_img_conf) # number of doubles/quads to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in [2]:\n",
    "    print(\"HST orbit(s): \", o)\n",
    "    doubles_mean = mean[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 2].iloc[:min_img_conf]\n",
    "    quads_mean = mean[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 4].iloc[:min_img_conf]\n",
    "    doubles_std = std[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 2].iloc[:min_img_conf]\n",
    "    quads_std = std[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 4].iloc[:min_img_conf]\n",
    "    doubles_truth = truth[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 2].iloc[:min_img_conf]\n",
    "    quads_truth = truth[o][chosen_dropout].loc[truth[o][chosen_dropout]['n_img'] == 4].iloc[:min_img_conf]\n",
    "    for param in ['lens_mass_center_x', 'src_light_center_x', 'lens_mass_center_y', 'src_light_center_y', 'lens_mass_gamma', 'lens_mass_theta_E', 'lens_mass_e1', 'lens_mass_e2', 'external_shear_gamma1', 'external_shear_gamma2', 'src_light_R_sersic']:\n",
    "        print(param)\n",
    "        print(\"Doubles\")\n",
    "        print(\"MAD: \", get_mad(doubles_mean[param].values, doubles_truth[param].values))\n",
    "        print(\"bias: \", get_weighted_bias(doubles_mean[param].values, doubles_std[param].values, doubles_truth[param].values))\n",
    "        print(\"precision: \", np.median(doubles_std[param].values))\n",
    "        print(\"Quads\")\n",
    "        print(\"MAD: \", get_mad(quads_mean[param].values, quads_truth[param].values))\n",
    "        print(\"bias: \", get_weighted_bias(quads_mean[param].values, quads_std[param].values, quads_truth[param].values))\n",
    "        print(\"precision: \", np.median(quads_std[param].values))\n",
    "        print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (baobab)",
   "language": "python",
   "name": "baobab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
